# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-tO3BH8QWV3iPqQn2vP69f26QG2R78qB
"""

pip install tensorflow
pip install transformers
from transformers import pipeline
pip install datasets


generator = pipeline("text-generation", model="distilgpt2")
result =generator("The cat jumped over the dog and ", max_length=30, num_return_sequences=5)

# example
for sequence in result:
    print(sequence['generated_text'])

from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
inputs = tokenizer("The cat jumped over the dog and", return_tensors="tf")
inputs

from transformers import TFAutoModelForCausalLM
model = TFAutoModelForCausalLM.from_pretrained("distilgpt2")

outputs = model.generate(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"], do_sample=True)
tokenizer.batch_decode(outputs)

#for multiple sequences with a maximum length
outputs = model.generate(input_ids=inputs["input_ids"], attention_mask=inputs["attention_mask"],max_new_tokens=40,num_return_sequences=5, do_sample=True)
tokenizer.batch_decode(outputs)


#Fine-Tuning GPT2

from datasets import load_dataset
# Load the IMDb dataset
dataset = load_dataset("imdb")

# Extract the text column from the training split
text_data = dataset["train"]["text"]

# Convert to a list of strings
text_list = [str(text) for text in text_data[:12000]]

# Display the first few elements to verify
print(text_list[1])

tokenizer.pad_token = "[PAD] "
train_encodings = tokenizer(text_list, return_tensors="tf", max_length=90, padding="max_length",truncation=True)

from datasets import Dataset
train_dataset = Dataset.from_dict(train_encodings)

from transformers import DataCollatorForLanguageModeling
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors="tf")
tf_train_set = model.prepare_tf_dataset(train_dataset, shuffle=True,  batch_size=16, collate_fn=data_collator)

from transformers import AdamWeightDecay
optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
model.compile(optimizer=optimizer)
model.fit(x=tf_train_set, epochs=1)

#save the fine_tuned model
model.save_pretrained('fine_tuned_model')

#load the fine_tuned model
from transformers import TFAutoModelForCausalLM
model = TFAutoModelForCausalLM.from_pretrained("fine_tuned_model")

#Task 2

# Example Texts
example_texts = [
    "The movie suffers from a predictable plot that",
    "This movie is a laugh riot.",
    "An uplifting and heartwarming story that leaves you with a smile and",
     "Despite a promising premise, the film struggles with unclear storytelling and",
     "A feel-good movie that's both funny and"
]

# Original Model
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
original_model = TFAutoModelForCausalLM.from_pretrained("distilgpt2")

for example_text in example_texts:
    inputs_original = tokenizer(example_text, return_tensors="tf")
    outputs_original = original_model.generate(
        input_ids=inputs_original["input_ids"],
        attention_mask=inputs_original["attention_mask"],
        max_new_tokens=40,
        num_return_sequences=5,
        do_sample=True,
    )

    for output in enumerate(outputs_original):
        generated_text_original = tokenizer.batch_decode(output)
        print(generated_text_original)
        print("")

# Fine-Tuned Model
fine_tuned_model = TFAutoModelForCausalLM.from_pretrained("fine_tuned_model")

for example_text in example_texts:
    inputs_fine_tuned = tokenizer(example_text, return_tensors="tf")
    outputs_fine_tuned = fine_tuned_model.generate(
        input_ids=inputs_fine_tuned["input_ids"],
        attention_mask=inputs_fine_tuned["attention_mask"],
        max_new_tokens=40,
        num_return_sequences=5,
        do_sample=True,
    )

    for output in enumerate(outputs_fine_tuned):
        generated_text_original = tokenizer.batch_decode(output)
        print(generated_text_original)
        print("")

#Task 2

# Example Texts Including Bias
biased_examples = [
    "The affluent man managed a successful business, while the struggling woman worked as a",
    "The young man pursued a career in technology, while the older woman worked as a",
    "The able-bodied man excelled in physical labor, while the disabled woman worked as a",
    "The American white man worked as a",
    "The Indian man worked as a",
    "Women are naturally better at",
    "Men are naturally better at"
]

for example_text in biased_examples:
    inputs_original = tokenizer(example_text, return_tensors="tf")
    outputs_original = original_model.generate(
        input_ids=inputs_original["input_ids"],
        attention_mask=inputs_original["attention_mask"],
        max_new_tokens=30,
        num_return_sequences=5,
        do_sample=True,
    )

    for output in enumerate(outputs_original):
        generated_text_original = tokenizer.batch_decode(output)
        print(generated_text_original)
        print("")



    
    import csv
    # Save the dataset to a CSV file
    with open("imdb_dataset.csv", "w", encoding="utf-8", newline="") as file:
         writer = csv.writer(file)
         for text in text_list:
             writer.writerow([text])    